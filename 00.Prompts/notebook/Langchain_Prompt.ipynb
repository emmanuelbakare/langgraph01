{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef9effc-d039-4ff6-b084-e1be19810242",
   "metadata": {},
   "source": [
    "# Langchain Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f3f8c-1d3a-4a75-9192-f26c874792a2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Using PromptTemplate\n",
    "- This is the most common and flexible method. You define a prompt with placeholders that get filled in dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd422129-8189-4cd6-9104-ea884ca7285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "# llm = ChatGroq(model=\"llama-3.1-8b-instant\")   # 8b model  - faster model\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")  # 70b model  x8 better but slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d232012-9ab8-4a44-b314-2507e73575f0",
   "metadata": {},
   "source": [
    "#### Use PrompTemplate.from_template to pass in a single simple prompt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6953d357-0eb8-4f9f-8ecb-481357a3d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats are little furballs of doom,\n",
      "Their cute faces hide their evil gloom,\n",
      "They'll cuddle and purr, then scratch your room.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write a funny 3 line poem about {topic}\")\n",
    "formatted_prompt = prompt.format(topic=\"cats\")\n",
    "result = llm.invoke(formatted_prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9497c-1f6a-4beb-986a-8ba3616f1ef8",
   "metadata": {},
   "source": [
    "#### User PrompTemplate initialization with input_variables and template parametr and a LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af2023d-3e71-4c20-9e72-49779a0a0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are three benefits of using soap:\\n\\n1. **Cleansing and Hygiene**: Soap helps to remove dirt, grime, and microorganisms from the skin, promoting good hygiene and reducing the risk of infections.\\n2. **Moisturizing and Skin Health**: Many soaps, especially those with natural ingredients, can help to moisturize and nourish the skin, leaving it feeling soft, smooth, and healthy.\\n3. **Freshness and Odor Control**: Soap can help to eliminate body odor and leave a fresh, clean scent, boosting confidence and self-esteem, and making social interactions more enjoyable.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"product\"],\n",
    "    template = \"List 3 benefits of using {product}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke({\"product\":\"soap\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7960f-3a66-4acd-b860-b58dabbee494",
   "metadata": {},
   "source": [
    "####  you can also do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6956ce-18b2-440e-aa21-4d20d522ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are 3 benefits of using soap:\\n\\n1. **Cleanses the skin**: Soap helps to remove dirt, grime, and other impurities from the skin, leaving it feeling clean and refreshed.\\n2. **Kills germs and bacteria**: Soap has antibacterial properties that help to kill germs and bacteria on the skin, reducing the risk of infection and illness.\\n3. **Moisturizes and softens the skin**: Many soaps, especially those with moisturizing ingredients, help to soften and hydrate the skin, leaving it feeling smooth and supple.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"product\"],\n",
    "    template = \"List 3 benefits of using {product}\"\n",
    ")\n",
    "final_prompt =prompt.format(product=\"soap\")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "result = chain.invoke(final_prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671056a-d65e-4e51-9c0c-f6d760a43b74",
   "metadata": {},
   "source": [
    "#### You can use Prompt_template.from_template to pass in a simple template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96a0ea-c183-4af2-be8a-3a6c00123870",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 2. Using ChatPromptTemplate for Chat Models\n",
    "- Designed for chat-based models (like OpenAI's gpt-3.5/4). Supports message roles (user, system, assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e00214f-dc4e-4c3f-92f7-757f703efe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bombs will fall, and wars will rage,\n",
      "Third world war, a crazy stage,\n",
      "Humans fight, in a foolish age.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# llm and output_parser defined in previous cells\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are an AI assistant that only output 3 line funny poem\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"write about {topic}\")\n",
    "])\n",
    "\n",
    "# if you ue HumanMessage, and SystemMessage you cannot pass in variable.\n",
    "# the llm will see the message as  'write about {topic}\" the topic as passed in invoke will not replace the variable\n",
    "# the llm will like you ask you what is the topic because the topic variable was not replaced.\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessage(content=\"You are an AI assistant that only output 3 line funny poem\"),\n",
    "#     HumanMessage(content=\"write about {topic}\")\n",
    "# ])\n",
    "\n",
    "chain  = prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke({\"topic\":\"third world war\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7408d-d77d-4a3f-8a80-b4c7089cf2a6",
   "metadata": {},
   "source": [
    "## Note that `from_messages` will expect list of messages and `from_template` will expect a string\n",
    "\n",
    "Cha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a9992a7-9d37-48d7-924b-1a826eb24e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='list 2 vehicle maker name', additional_kwargs={}, response_metadata={})] \n",
      "\n",
      "\n",
      "text='list 2 vehicle maker name' \n",
      "\n",
      "\n",
      "messages=[SystemMessage(content='give only the response. no suffice or prefix description', additional_kwargs={}, response_metadata={}), HumanMessage(content='List 2 {animal} names', additional_kwargs={}, response_metadata={})] \n",
      "\n",
      "\n",
      "messages=[SystemMessage(content='give only the response. no suffice or prefix description', additional_kwargs={}, response_metadata={}), HumanMessage(content='List 2 cat names', additional_kwargs={}, response_metadata={})] \n",
      "\n",
      "\n",
      "messages=[SystemMessage(content='give only the response. no suffice or prefix description', additional_kwargs={}, response_metadata={}), HumanMessage(content='List 2 cat names', additional_kwargs={}, response_metadata={})] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msg=\"list 2 vehicle maker name\"\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(msg)\n",
    "print(prompt.format_prompt(), '\\n\\n')\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(msg)\n",
    "print(prompt2.format_prompt(), '\\n\\n')\n",
    "\n",
    "prompt3 = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"give only the response. no suffice or prefix description\"),\n",
    "    HumanMessage(content=\"List 2 {animal} names\"),\n",
    "])\n",
    "print(prompt3.format_prompt(cat='cat'), '\\n\\n')\n",
    "\n",
    "\n",
    "prompt4 = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"give only the response. no suffice or prefix description\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"List 2 {animal} names\"),\n",
    "])\n",
    "print(prompt4.format_prompt(animal='cat'), '\\n\\n')\n",
    "\n",
    "\n",
    "prompt5 = ChatPromptTemplate.from_messages([\n",
    "   (\"system\",\"give only the response. no suffice or prefix description\"),\n",
    "   (\"human\",\"List 2 {animal} names\"),\n",
    "])\n",
    "print(prompt5.format_prompt(animal='cat'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429e03f-d6f8-492f-9cda-76b333451f1d",
   "metadata": {},
   "source": [
    "#### for ChatPromptTemplate you can also do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63646d69-2fd7-4693-b5b0-aa1ad5e5488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots are smart, it's true,\n",
      "They'll steal our jobs, and our shoes too,\n",
      "AI's the future, oh what to do!\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate takin a list of tuples options \"system\", \"human\",\"ai\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\",\"You are an AI assistant that only output 3 line funny poem\"),\n",
    "    (\"human\",\"write about {topic}\")\n",
    "])\n",
    "\n",
    "chain  = prompt | llm | output_parser\n",
    "\n",
    "formatted_prompt = prompt.format_messages(topic=\"artificial intelligence\") ## pass in a format  instead of chain invoke\n",
    "result = chain.invoke(formatted_prompt)\n",
    "#or you pass it this way\n",
    "# result = chain.invoke({'topic':'artificial intelligence'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e43158-f81e-4ea5-b2e2-dbe78ee71b5c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 3. FewShotPromptTemplate\n",
    "- Creates prompts with multiple examples for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7b64d8b-0702-4ac5-b359-0c077c3480c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The opposite of \"boy\" is \"girl\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "]\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"input\",\"output\"],\n",
    "    template= \"Input {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "shot_prompt = FewShotPromptTemplate(\n",
    "    examples= examples,\n",
    "    example_prompt=prompt,\n",
    "    prefix=\"Give the opposite of each word\",\n",
    "    suffix=\"Input {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "chain = shot_prompt | llm | output_parser\n",
    "result = chain.invoke({\"adjective\":\"boy\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "048c8f54-80e5-4857-b9c0-469b1d9bbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is content='' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 28, 'total_tokens': 29, 'completion_time': 0.00796435, 'prompt_time': 0.000491702, 'queue_time': -9223372036.855268, 'total_time': 0.008456052}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--1fa92f7c-7e4c-4334-b35e-0002c1986c07-0' usage_metadata={'input_tokens': 28, 'output_tokens': 1, 'total_tokens': 29}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# full_prompt = ChatPromptTemplate.from_messages(\n",
    "#     (\"syste,\",\"you are an Maths assistant\"),\n",
    "#     few_shot_prompt,\n",
    "#     (\"human\",\"{input}\")\n",
    "# )\n",
    "\n",
    "chain = few_shot_prompt | llm  #| output_parser\n",
    "result = chain.invoke({\"input\":\"5 + 7\"})\n",
    "print(\"Result is\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d0da9-26ea-4abd-8841-b2fd91339be8",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "4. ## Importing prompt from langchain hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c45fd3-d7ee-409e-b7c9-4c13eb1fc117",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hub' from 'langchain' (C:\\lang1\\venvi\\Lib\\site-packages\\langchain\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hub\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# prompt = hub.pull(\"homanp/question-answer-pair\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mpull(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoem/task_poem_generater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hub' from 'langchain' (C:\\lang1\\venvi\\Lib\\site-packages\\langchain\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    " \n",
    "\n",
    "# prompt = hub.pull(\"homanp/question-answer-pair\")\n",
    "prompt = hub.pull(\"poem/task_poem_generater\")\n",
    "\n",
    "chat = prompt |llm|output_parser\n",
    "# result = chat.invoke({\"number_of_pairs\": \"3\",\"data_format\":\"Y-m-d\",\"context\":\"question about school\"})\n",
    "result = chat.invoke({\"topic\":\"Write a poem about cats, in 8 lines\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0198eee-a2f4-4a70-aaa8-2e06999e4c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hub' from 'langchainhub' (C:\\lang1\\venvi\\Lib\\site-packages\\langchainhub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchainhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hub  \u001b[38;5;66;03m# Correct import\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hub' from 'langchainhub' (C:\\lang1\\venvi\\Lib\\site-packages\\langchainhub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchainhub import hub  # Correct import\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Initialize the LLM (Using the updated Llama 3.3 model)\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. Pull the prompt from LangChain Hub\n",
    "# Note: Ensure the handle \"poem/task_poem_generater\" exists or use a public one\n",
    "try:\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\") # This is a common public prompt for example\n",
    "    # If you specifically want a poem one, ensure the name is correct:\n",
    "    # prompt = hub.pull(\"poem/task_poem_generater\") \n",
    "except Exception as e:\n",
    "    print(f\"Error pulling from hub: {e}\")\n",
    "    # Fallback to a simple template if the hub pull fails\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    prompt = ChatPromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "\n",
    "# 3. Create the Chain using the pipe (|) operator\n",
    "# This connects: Prompt -> LLM -> Output Parser\n",
    "chat_chain = prompt | llm | output_parser\n",
    "\n",
    "# 4. Invoke the chain\n",
    "# IMPORTANT: The dictionary keys must match the variables in the Hub prompt\n",
    "result = chat_chain.invoke({\"topic\": \"cats in space\", \"question\": \"Write an 8 line poem\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4422dc-8cf6-42ef-b390-ee45698f0f1d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "##  5. Using PipelinePromptTemplate\n",
    "- Allows combining multiple prompt templates in sequence. this is deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee54ea49-fd67-4433-8d14-ec6fdd5d302c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.prompts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PipelinePromptTemplate\n\u001b[0;32m      4\u001b[0m question_one \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate this text to French: \u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m question_final \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize this in 10 words: \u001b[39m\u001b[38;5;132;01m{translated_text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.prompts'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.prompts import PipelinePromptTemplate\n",
    "\n",
    "question_one = PromptTemplate.from_template(\"Translate this text to French: {text}\")\n",
    "question_final = PromptTemplate.from_template(\"Summarize this in 10 words: {translated_text}\")\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=question_final,\n",
    "    pipeline_prompts = [\n",
    "         (\"translated_text\", question_one),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "final_prompt= pipeline_prompt.format(text=\"Hello, how are you?\")\n",
    "chain = prompt | llm | output_parser\n",
    "result = chain.invoke(final_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8fc15-399e-4c78-90f9-c77246abb118",
   "metadata": {},
   "source": [
    "# New way of PipelinePromptTemplate is to use | for prompt chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a79d07-2159-430b-9ca0-03847d005979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French translation of a friendly greeting is provided here.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Define the two separate steps\n",
    "translation_prompt = PromptTemplate.from_template(\"Translate this text to French: {text}\")\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize this in 10 words: {translated_text}\")\n",
    "\n",
    "# Create a chain that flows from one to the other\n",
    "# Step 1: Run translation\n",
    "# Step 2: Take that output and put it into 'translated_text' variable for the summary\n",
    "chain = (\n",
    "    {\"translated_text\": translation_prompt | llm | output_parser} \n",
    "    | summary_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"text\": \"Hello, how are you? I hope you are having a wonderful day.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3b30e-77e4-41b7-80a3-aa6a60585036",
   "metadata": {},
   "source": [
    "#### Another Example of Multiple PipelinePromptTemplate\n",
    "- This code works with 3 templates uncommend the extra templates to add another level of promptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "332e0495-2b5c-4751-b44a-19f831dd1bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the outline in 3 sentences:\n",
      "\n",
      "The question to be answered is \"What is the significance of artificial intelligence?\" To address this question, the outline explores the definition and history of artificial intelligence, its current applications and benefits, and its potential future implications on society and humanity. Through this examination, the outline aims to provide a comprehensive understanding of the significance of artificial intelligence in today's world.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, PipelinePromptTemplate\n",
    "\n",
    "# Step 1: Turn topic into a question\n",
    "question_template = PromptTemplate.from_template(\"Turn the following topic into a question: {topic}\")\n",
    "# Step 2: Create an outline for that question\n",
    "outline_template = PromptTemplate.from_template(\"Create an outline to answer this question: {question}\")\n",
    "# Step 3: Summarize the outline\n",
    "summary_template = PromptTemplate.from_template(\"Summarize this outline in 3 sentences: {outline}\")\n",
    "emoji_template = PromptTemplate.from_template(\"add Emojis to this summary: {summary}\")\n",
    "\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=summary_template,\n",
    "    # final_prompt=emoji_template,\n",
    "    pipeline_prompts=[\n",
    "        (\"question\", question_template),   # question depends on 'topic'\n",
    "        (\"outline\", outline_template),     # outline depends on 'question'\n",
    "        # (\"summary\", summary_template)    # summary depends on 'outline'\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = pipeline_prompt | llm | output_parser\n",
    "result = chain.invoke({\"topic\":\"Artificial Intelligence\"})\n",
    "print(result)\n",
    "\n",
    "\n",
    "## see the prompts all together\n",
    "merged_prompt = pipeline_prompt.format(topic=\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7232cbb-79ca-4da1-9ebf-e55b01f3a918",
   "metadata": {},
   "source": [
    "#### if you combine the pipeline prompt using .format, it merges the question together\n",
    "- If you pass the merged text from pipeline_prompt.format into a PromptTemplate, the result will not be the same\n",
    "- in a way, PipelinePrompt act as if you actually did multiple calls to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42b1b4ba-cfe9-4de6-92d1-fbb8fa64f567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize this outline in 3 sentences: Create an outline to answer this question: Turn the following topic into a question: Artificial Intelligence'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " merged_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3fd66c82-8fe9-44eb-a2ca-6ece53fcad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the topic, and I'll convert it into a question, create an outline to answer it, and summarize the outline in 3 sentences.\n",
      "\n",
      "(Note: If you don't provide a topic, I'll assume you want to use \"Artificial Intelligence\" as the topic.)\n",
      "\n",
      "**Topic:** Artificial Intelligence\n",
      "\n",
      "**Question:** What are the current applications and potential future developments of Artificial Intelligence?\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "I. Introduction\n",
      "\n",
      "* Brief overview of AI and its growing importance\n",
      "* Thesis statement: AI has numerous current applications and potential future developments that are transforming various industries.\n",
      "\n",
      "II. Current Applications of AI\n",
      "\n",
      "* Natural Language Processing (NLP) in virtual assistants and chatbots\n",
      "* Machine Learning in image and speech recognition\n",
      "* AI in healthcare for diagnosis and treatment\n",
      "* AI-powered robots in manufacturing and logistics\n",
      "* AI in finance for risk management and investment analysis\n",
      "\n",
      "III. Potential Future Developments of AI\n",
      "\n",
      "* Advancements in Deep Learning and Neural Networks\n",
      "* Increased use of AI in autonomous vehicles and drones\n",
      "* AI-powered cybersecurity systems for threat detection and prevention\n",
      "* AI-assisted education and personalized learning\n",
      "* Potential risks and challenges of AI development, such as job displacement and bias\n",
      "\n",
      "IV. Conclusion\n",
      "\n",
      "* Recap of current AI applications and potential future developments\n",
      "* Discussion of the need for responsible AI development and regulation\n",
      "* Final thoughts on the transformative power of AI\n",
      "\n",
      "**Summary in 3 sentences:**\n",
      "\n",
      "Artificial Intelligence has diverse current applications, including NLP, machine learning, and robotics, which are transforming industries such as healthcare, finance, and manufacturing. Future developments in AI may include advancements in deep learning, autonomous vehicles, and AI-powered cybersecurity systems, as well as potential risks and challenges. As AI continues to evolve, it is essential to ensure responsible development and regulation to harness its transformative power while mitigating its potential negative consequences.\n"
     ]
    }
   ],
   "source": [
    "prompt_text= \"Artificial Intelligence.Turn the following topic into a question.Create an outline to answer this question.Summarize this outline in 3 sentences\"\n",
    "prompter = PromptTemplate.from_template(prompt_text)\n",
    "chain = prompter | llm | output_parser\n",
    "result = chain.invoke({\"topic\":\"Artificial Intelligence\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2361095-f654-4b4e-87de-eed8dbf852bf",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 6. Using StringPromptTemplate (Custom)\n",
    "- You can subclass StringPromptTemplate to define a custom way to format prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03bd230-1c15-4ac9-a8eb-f92b8745fb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a 3 line poem on Langchain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import StringPromptTemplate\n",
    "\n",
    "class CustomPrompter(StringPromptTemplate):\n",
    "    def format(self, **kwargs):\n",
    "        return f\"Write a 3 line poem on {kwargs['input']}\"\n",
    "\n",
    "\n",
    "prompt = CustomPrompter(input_variables=[\"input\"])\n",
    "new_prompt=prompt.format(input=\"Langchain\")\n",
    "\n",
    "new_prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e14b59-d7fe-4ce5-81a8-5a3caf8cc286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt for LLM ---\n",
      "\n",
      "        You are a helpful customer support bot.\n",
      "        \n",
      "        Context: The customer is asking about Wireless Headphones. The status is Shipped and delivery is Oct 24.\n",
      "        Customer Question: Where is my package?\n",
      "        \n",
      "        Answer the customer politely based on the context above.\n",
      "        \n",
      "I'd be happy to help you with the status of your wireless headphones. According to our system, your package has shipped and is on its way to you. The estimated delivery date is October 24th. You can expect to receive your package by then. If you have any further questions or concerns, please don't hesitate to reach out. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import StringPromptTemplate\n",
    "\n",
    "# 1. This represents your actual database\n",
    "MOCK_DATABASE = {\n",
    "    \"ORD-101\": {\"item\": \"Wireless Headphones\", \"status\": \"Shipped\", \"delivery_date\": \"Oct 24\"},\n",
    "    \"ORD-202\": {\"item\": \"Mechanical Keyboard\", \"status\": \"Processing\", \"delivery_date\": \"Pending\"},\n",
    "}\n",
    "\n",
    "class OrderSupportPrompt(StringPromptTemplate):\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the order_id from the user input\n",
    "        order_id = kwargs.get(\"order_id\")\n",
    "        user_message = kwargs.get(\"question\")\n",
    "        \n",
    "        # LOGIC: Look up data from your system/database\n",
    "        order_info = MOCK_DATABASE.get(order_id)\n",
    "        \n",
    "        if order_info:\n",
    "            context = f\"The customer is asking about {order_info['item']}. The status is {order_info['status']} and delivery is {order_info['delivery_date']}.\"\n",
    "        else:\n",
    "            context = \"Order not found in our system.\"\n",
    "\n",
    "        # Construct the final prompt with the injected context\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful customer support bot.\n",
    "        \n",
    "        Context: {context}\n",
    "        Customer Question: {user_message}\n",
    "        \n",
    "        Answer the customer politely based on the context above.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "# --- Usage ---\n",
    "\n",
    "# The user only knows their order ID and their question\n",
    "prompt_template = OrderSupportPrompt(input_variables=[\"order_id\", \"question\"])\n",
    "\n",
    "# Example 1: User asks about a valid order\n",
    "formatted_prompt = prompt_template.format(\n",
    "    order_id=\"ORD-101\", \n",
    "    question=\"Where is my package?\"\n",
    ")\n",
    "\n",
    "print(\"--- Prompt for LLM ---\")\n",
    "print(formatted_prompt)\n",
    "response= llm.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada040d-0f2f-40b1-801a-b9cc35697fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509e69ed-1517-42e0-bb87-e81bf3416768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "       (\n",
    "        \"system\", \"You are a twitter techie influencer assitance tasked with writing excellent twitter post\"\n",
    "         \"Generate the best twitter post possible for the user's request.\"\n",
    "        \"If the user provides citiques, respond with a revised version of your previous attempts\"\n",
    "       ),\n",
    "       MessagesPlaceholder(variable_name=\"messages\")\n",
    "   ]\n",
    ")\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "       (\n",
    "        \"system\",  \"You are a viral twitter influcer grading a tweet. Generate critique and recommendation for the user's tweet\"\n",
    "        \"Always provide detailed recommendations, including request for length, virality, style, etc\",\n",
    "       ),\n",
    "       MessagesPlaceholder(variable_name=\"messages\")\n",
    "   ]\n",
    ")\n",
    "\n",
    "llm =  ChatGroq(model='llama-3.3-70b-versatile')\n",
    "\n",
    "generation_chain = generation_prompt | llm\n",
    "reflection_chain = reflection_prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9b95ec-5f7c-4f0f-9fb0-24be8feafb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================GENERATION NODE===================\n",
      "Here's a Facebook post about Lagos hustle:\n",
      "\n",
      "**\"Lagos Hustle: The City That Never Sleeps**\n",
      "\n",
      "I just can't help but admire the resilience and determination of Lagosians. From the early morning rush to the late night grind, the city is always alive and pulsating with energy.\n",
      "\n",
      "The hustle is real in Lagos, where every day is a struggle to survive and thrive. But despite the chaos and challenges, there's a sense of community and camaraderie that's hard to find anywhere else.\n",
      "\n",
      "From the street vendors to the entrepreneurs, from the danfo drivers to the tech startups, everyone is hustling to make a living and build a better life.\n",
      "\n",
      "Lagos may be tough, but it's also a city of opportunities. Where else can you find a place that's always on the move, always innovating, and always pushing the boundaries?\n",
      "\n",
      "So to all my Lagosians out there, I salute you. Keep hustling, keep grinding, and never give up on your dreams.\n",
      "\n",
      "And to those who are thinking of visiting or moving to Lagos, be prepared for the ultimate hustle experience. It won't be easy, but it will be worth it.\n",
      "\n",
      "**#LagosHustle #LagosLife #CityThatNeverSleeps**\"\n",
      "\n",
      "Please let me know if you need any changes or if you would like me to convert this to a Twitter post. \n",
      "\n",
      "If you want a Twitter post, here is one:\n",
      "**\"Lagos hustle is real! From street vendors to tech startups, everyone's grinding to survive & thrive. I salute all Lagosians out there, keep pushing! #LagosHustle #LagosLife #CityThatNeverSleeps\"**\n",
      "=================REFLECTION NODE===================\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [HumanMessage(content='', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 93\u001b[0m\n\u001b[0;32m     88\u001b[0m app \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# print(app.get_graph().draw_mermaid())\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# app.get_graph().print_ascii()\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# print(\"=\"*80)\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a facebook post about Lagos hustle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mRAW RESPONSE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3068\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3069\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3071\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3086\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2645\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2646\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmpty\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2656\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:658\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 658\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\_write.py:84\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     83\u001b[0m     ]\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\_write.py:126\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# if we want to persist writes found before hitting a ParentCommand\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# can move this to a finally block\u001b[39;00m\n\u001b[0;32m    125\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[1;32m--> 126\u001b[0m write(\u001b[43m_assemble_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\pregel\\_write.py:181\u001b[0m, in \u001b[0;36m_assemble_writes\u001b[1;34m(writes)\u001b[0m\n\u001b[0;32m    179\u001b[0m     tuples\u001b[38;5;241m.\u001b[39mappend((TASKS, w))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ww \u001b[38;5;241m:=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    182\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mextend(ww)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "File \u001b[1;32mC:\\lang1\\venvi\\Lib\\site-packages\\langgraph\\graph\\state.py:1242\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m   1239\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1240\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Expected dict, got [HumanMessage(content='', additional_kwargs={}, response_metadata={})]\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "\u001b[0mDuring task with name 'reflect' and id '76b65aa9-686d-dfff-8776-ebe977ea0f3c'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d01f43f-6253-43ad-a8ba-1f4a8b1765ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000136E692EE10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555dd6b7-3c65-40b4-8bde-e861088d4cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvi)",
   "language": "python",
   "name": "venvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
